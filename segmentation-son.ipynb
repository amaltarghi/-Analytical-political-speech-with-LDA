{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import pattern.fr\n",
    "\n",
    "#import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stoplist = get_stop_words('fr')\n",
    "stoplist2 = get_stop_words('en')\n",
    "\n",
    "stoplist.extend(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}','#','-','...','..',':)',';)','plus','via','contre'\n",
    "                 ,'will','now','today','can','day','just','new','apres','bien','aujourd','hui'])\n",
    "stoplist.extend(stoplist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_token_tweet(tweet):\n",
    "    lst = tknzr.tokenize(tweet)\n",
    "    lst2 = []\n",
    "    for i in range(len(lst)):\n",
    "        if not(lst[i][:4]=='http' or  lst[i][:1]=='@' or re.match(emoji_pattern,lst[i][:])!=None):\n",
    "            lst2.append(lst[i])\n",
    "    return \" \".join(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def segment_french(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lst = [word for word in tokens if word not in stoplist and len(word)>=3]\n",
    "    sentence = \" \".join(lst)\n",
    "    normal = unicodedata.normalize('NFKD', unicode(sentence)).encode('ASCII', 'ignore')\n",
    "    return normal.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    return segment_french(get_token_tweet(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "directory = os.path.join(\".\",\"out2/\")\n",
    "cnt = 0\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        cnt += 1\n",
    "        if file.endswith(\".csv\"):\n",
    "            #print root+'/'+file\n",
    "            df = pd.read_csv(root+'/'+file,sep='\\t',encoding='utf-8')\n",
    "            text_part = []\n",
    "            for txt in df['TEXT']:\n",
    "                text_part.extend(process(txt.lower()))\n",
    "        text.append(text_part)\n",
    "        if(cnt%1000==0):\n",
    "            print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text3 = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot compute LDA over an empty collection (no terms)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-99e409e5afd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/ldamodel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute LDA over an empty collection (no terms)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute LDA over an empty collection (no terms)"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "dictionary = corpora.Dictionary(text3)\n",
    "corpus = [dictionary.doc2bow(txt) for txt in text3]\n",
    "          \n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, update_every=1, chunksize=10000, passes=1)\n",
    "lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "prepare_data = pyLDAvis.gensim.prepare(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr_js = prepare_data.to_dict()\n",
    "import json\n",
    "\n",
    "with open(\"out-last.json\", 'wb') as outfile:\n",
    "    json.dump(pr_js, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''[(0,\n",
    "  u'0.027*\"issy\" + 0.009*\"maire\" + 0.005*\"fusion\" + 0.005*\"eelv\" + 0.005*\"conseil\" + 0.005*\"prg\" + 0.005*\"boulogne\" + 0.004*\"municipal\" + 0.004*\"levallois\" + 0.004*\"c\"'),\n",
    " (1,\n",
    "  u'0.019*\"loi\" + 0.013*\"contre\" + 0.009*\"s\\xe9nat\" + 0.009*\"directan\" + 0.008*\"projet\" + 0.006*\"opendata\" + 0.005*\"commission\" + 0.005*\"c\" + 0.005*\"vote\" + 0.004*\"/\"'),\n",
    " (2,\n",
    "  u'0.022*\"cgt\" + 0.021*\"loitravail\" + 0.016*\"elkhomri\" + 0.012*\"c\" + 0.007*\"contre\" + 0.006*\"valls\" + 0.006*\"plus\" + 0.006*\"nuitdebout\" + 0.005*\"gauche\" + 0.005*\"ovpl\"'),\n",
    " (3,\n",
    "  u'0.018*\"rousset2015\" + 0.008*\"regionales2015\" + 0.006*\"toulouse\" + 0.006*\"r\\xe9gion\" + 0.005*\"montpellier\" + 0.005*\"lrmp\" + 0.005*\"innovation\" + 0.004*\"c\" + 0.004*\"via\" + 0.004*\"visite\"'),\n",
    " (4,\n",
    "  u'0.156*\"trump\" + 0.028*\"clinton\" + 0.017*\"donald\" + 0.011*\"electionnight\" + 0.010*\"volkswagen\" + 0.009*\"usa\" + 0.008*\"victoire\" + 0.007*\"hillary\" + 0.007*\"\\xe9lection\" + 0.005*\"unis\"'),\n",
    " (5,\n",
    "  u'0.013*\"c\" + 0.013*\"sarkozy\" + 0.013*\"fillon\" + 0.011*\"france\" + 0.009*\"hollande\" + 0.007*\"plus\" + 0.006*\"primaireledebat\" + 0.006*\"primairedroite\" + 0.006*\"jupp\\xe9\" + 0.005*\"valls\"'),\n",
    " (6,\n",
    "  u'0.027*\"via\" + 0.011*\"merci\" + 0.010*\"streetart\" + 0.008*\"paris\" + 0.007*\"panamapapers\" + 0.007*\"art\" + 0.006*\"web\" + 0.006*\"num\\xe9rique\" + 0.005*\"compte\" + 0.005*\"france\"'),\n",
    " (7,\n",
    "  u'0.021*\"arras\" + 0.011*\"cashinvestigation\" + 0.007*\"14juillet\" + 0.007*\"c\" + 0.006*\"1\" + 0.006*\"0\" + 0.005*\"poissy\" + 0.005*\"havre\" + 0.004*\"tours\" + 0.004*\"2\"'),\n",
    " (8,\n",
    "  u'0.015*\"paris\" + 0.006*\"verdun\" + 0.006*\"ville\" + 0.006*\"paris12\" + 0.005*\"c\" + 0.005*\"merci\" + 0.005*\"/\" + 0.004*\"soir\" + 0.004*\"2016\" + 0.004*\"inauguration\"'),\n",
    " (9,\n",
    "  u'0.015*\"france\" + 0.015*\"europe\" + 0.012*\"brexit\" + 0.011*\"ue\" + 0.007*\"migrants\" + 0.007*\"ruquierdegage\" + 0.006*\"mossoul\" + 0.005*\"plus\" + 0.005*\"s\" + 0.005*\"turquie\"'),\n",
    " (10,\n",
    "  u'0.044*\"migrants\" + 0.020*\"bretagne\" + 0.018*\"calais\" + 0.013*\"paris\" + 0.012*\"ledrian\" + 0.009*\"rennes\" + 0.007*\"brest\" + 0.006*\"via\" + 0.005*\"france\" + 0.005*\"r\\xe9fugi\\xe9s\"'),\n",
    " (11,\n",
    "  u'0.027*\"euro2016\" + 0.015*\"rio2016\" + 0.015*\"france\" + 0.011*\"bravo\" + 0.009*\"fra\" + 0.009*\"finale\" + 0.008*\"strasbourg\" + 0.007*\"match\" + 0.006*\"victoire\" + 0.006*\"c\"'),\n",
    " (12,\n",
    "  u'0.023*\"regionales2015\" + 0.014*\"fn\" + 0.008*\"r\\xe9gionales2015\" + 0.008*\"r\\xe9gion\" + 0.008*\"emploi\" + 0.006*\"plus\" + 0.006*\"c\" + 0.006*\"rsi\" + 0.006*\"gauche\" + 0.005*\"france\"'),\n",
    " (13,\n",
    "  u'0.039*\"cop21\" + 0.015*\"climat\" + 0.013*\"pesticides\" + 0.007*\"paris\" + 0.006*\"via\" + 0.006*\"france\" + 0.005*\"c\" + 0.005*\"accorddeparis\" + 0.005*\"plus\" + 0.005*\"accord\"'),\n",
    " (14,\n",
    "  u'0.041*\"the\" + 0.014*\"for\" + 0.014*\"and\" + 0.009*\"s\" + 0.007*\"with\" + 0.006*\"from\" + 0.005*\"you\" + 0.005*\"are\" + 0.005*\"deezer\" + 0.004*\"new\"'),\n",
    " (15,\n",
    "  u'0.007*\"raid\" + 0.007*\"france\" + 0.007*\"victimes\" + 0.007*\"soutien\" + 0.006*\"policiers\" + 0.006*\"nantes\" + 0.006*\"contre\" + 0.006*\"agriculteurs\" + 0.006*\"agriculture\" + 0.005*\"saintdenis\"'),\n",
    " (16,\n",
    "  u'0.006*\"avecbarto\" + 0.005*\"jeunes\" + 0.005*\"c\" + 0.005*\"idf\" + 0.004*\"france\" + 0.004*\"lyon\" + 0.004*\"soir\" + 0.004*\"merci\" + 0.004*\"matin\" + 0.004*\"regionales2015\"'),\n",
    " (17,\n",
    "  u'0.020*\"cannes\" + 0.019*\"courbevoie\" + 0.018*\"starwars\" + 0.012*\"puteaux\" + 0.007*\"hautsdeseine\" + 0.006*\"film\" + 0.004*\"metz\" + 0.004*\"angers\" + 0.004*\"cotedazurnow\" + 0.004*\"gravelines\"'),\n",
    " (18,\n",
    "  u'0.008*\"sncf\" + 0.007*\"grenoble\" + 0.006*\"coll\\xe8ge\" + 0.006*\"france\" + 0.004*\"c\" + 0.004*\"sant\\xe9\" + 0.004*\"coll\\xe8ge2016\" + 0.004*\"plus\" + 0.004*\"pollution\" + 0.003*\"coll\\xe8ges\"'),\n",
    " (19,\n",
    "  u'0.018*\"avecbarto\" + 0.009*\"hommage\" + 0.009*\"bataclan\" + 0.007*\"france\" + 0.006*\"soir\" + 0.006*\"parisattacks\" + 0.006*\"paris\" + 0.005*\"attentatsparis\" + 0.005*\"victimes\" + 0.004*\"pantin\"')]'''\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
